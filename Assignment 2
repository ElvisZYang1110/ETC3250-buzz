---
title: "ETC3250/5250 Assignment 2"
date: 'DUE: Friday April 30, 2021, 11:55pm'
output:
  html_document:
    after_body: tutorial-footer.html
  pdf_document: default
---

```{r echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  echo = FALSE,
  comment = "#",
  fig.height = 4,
  fig.width = 8,
  fig.align = "center",
  cache = FALSE
)
```

## Learning objectives being assessed

- Select and develop appropriate models
- Estimate and simulate, and use resampling to measure uncertainty
- Explain and interpret the analyses


## Instructions

- This is an **individual** assignment. It is expected that you complete this assignment individually, without discussing with other people, including students in this unit, or posting questions to help sites. You can use the textbook, lecture notes, and any material openly available on the web (appropriately acknowledged) as needed. You are encouraged to seek help from your tutor or lecturer - be aware, that they can help you get past any hurdles and thinking about the problems but will not provide specific answers to the assignment questions.
- Assignment needs to be turned in as Rmarkdown, and as html, to moodle. That is, **two files** need to be submitted.
- You can write your solution by hand to parts of Q3b, if you prefer, and add the image to your Rmd, or upload it as a separate file. 
- R code should be hidden in the final report, unless it is specifically requested.
- Original work is expected. Any material used from external sources needs to be acknowledged. 
- NO LATE SUBMISSIONS WILL BE ACCEPTED, and please be aware that help after 5pm on due date cannot be expected. The late time is given as a convenience for students working full-time jobs, who may need a little after working hours time to wrap up small details. You should aim to submit your assignment in advance of the deadline.


## Marks

- Total mark will be out or 25
- 2 points will be reserved for readability, and appropriate citing of external sources 
- 1.5 points will be reserved for reproducibility, that the report can be re-generated from the submitted Rmarkdown. 
- 1.5 points reserved for clean and readable code.
- Accuracy and completeness of answers, and clarity of explanations will be the basis for the remaining 20 points. 

## Exercises

# 1. (4pts) This question examines the boundaries for different models. 

The lecture notes provide code to plot the QDA boundary between regions 2 and 3 of the olive oils data, using linoleic and arachidic acid. Use the same split of training and testing sets as used in lecture notes.

```{r,echo=FALSE}


#esults='hide', fig.show='hide', eval=FALSE
library(tidyverse)
library(tidymodels)
library(discrim)
library(MASS)
olive <- read_csv("http://ggobi.org/book/data/olive.csv") %>%
  dplyr::filter(region != 1) %>%
  dplyr::select(region, arachidic, linoleic) %>%
  dplyr::mutate(region = factor(region))




#standardization
std<-function(x) (x-mean(x,na.rm=T))/sd(x, na.rm=T)
olive_std <-olive %>% 
  mutate_if(is.numeric,std)





```



**The dot plot of the overall data without standardization **

```{r}
ggplot(olive, aes(x=arachidic, y=linoleic, colour=region)) +
  geom_point() + 
  scale_colour_brewer(palette="Dark2")


```


```{r} 

set.seed(775)
olive_split <- initial_split(olive_std, 2/3, strata = region)
olive_train <- analysis(olive_split)
olive_test <- assessment(olive_split)

```





**Here we can see the plot of the olive_test data after the standardization **

>olive test 

```{r}

ggplot(olive_test, aes(x=arachidic, y=linoleic, colour=region)) +
  geom_point() + 
  scale_colour_brewer(palette="Dark2")

# Standardize the data 
```

>olive Train

```{r}

ggplot(olive_train, aes(x=arachidic, y=linoleic, colour=region)) +
  geom_point() + 
  scale_colour_brewer(palette="Dark2")

```



### a.(1pt) Write down the equation for discriminant function for region 2 as specified by QDA, using values computed from the data. (Use standardised variables.) Also, explain what assumption is not satisfied for QDA to be correctly applied for this data.



**Under the QDA rule, the following equation are for discriminant function **
$$
\delta_k(x) = x^T \Sigma_k^{-1}x+x^T\Sigma_k^{-1}\mu_k- \frac12\mu_k^T\Sigma_k^{-1}\mu_k-\frac12\log{|\Sigma_k|}+\log(\pi_k)
$$



**FOr the formula of y**
$$
\delta_k(y)=y^Ty+y^T\eta_k-\frac12\eta_k^T\eta_k+log(\pi_k)
$$

$$
y=A_kx
$$




**According to the data output showed below, we can get the discriminant function for region 2 is **

$$
\delta_2(x) = x^T \Sigma_2^{-1}x+x^T\Sigma_2^{-1}\mu_2- \frac12\mu_2^T\Sigma_2^{-1}\mu_2-\frac12\log{|\Sigma_2|}+\log(\pi_2)
$$


**where in the following output we can get that  **
$$
\pi_k = 0.3952096,\mu_2=[0.7483237,1.0746306]^T
$$
Then 
$$
\Sigma _k \ and\ log{|\Sigma_k|}
$$ 

will be 

$$
\begin{aligned}
\log{|\Sigma_2|}&=-3.497169\\

\Sigma_2^{-1}&=\begin{bmatrix}5.59796105&0.04295803\\0.04295803&5.89923100\end{bmatrix}\\
\log(\pi_2)=0.3952096\\
\Sigma_2&=\begin{bmatrix}0.178646453&-0.001300898\\-0.001300898&0.169523093\end{bmatrix}\\
\pi_2=0.3952096\\

\end{aligned}
$$



> Under the QDA rule, the data should be a equal variance-covariance.
In this question , It's an unequal variance and covariance, so here we can conclude that the rule is not suitable in this case. 

>Moreover, the distribution of the observation in each class shld be a normal distribution with a class mean and class ocvariance. As for this question the 2 variables are not normal distribution instead of a multimodal distribtion. Thus, olive data cannot satisfied the assumption 




```{r,echo=FALSE}

library(MASS)
olive_qda_fit <- qda(region ~ ., 
      data = olive_train)
olive_qda_fit




olive_region2 <- olive_train %>%
  filter(region == "2")

cov(olive_region2[,2:3])
```







### b.(1pt) Make similar plots that shows the boundary between the two regions, that would be produced by .

***(i)* linear discriminant analysis, **



```{r,echo=FALSE}


lda_mod <- discrim_linear() %>% 
  set_engine("MASS") %>% 
  translate()


olive_lda_fit <- 
  lda_mod %>% 
  fit(region ~ ., 
      data = olive_train)

olive_lda_fit


# Training Error

```


**The Linear Discriminant Matrix of the Train data **
```{r}
library(kableExtra)
olive_lda_tr_pred <- olive_train %>%
  mutate(.pred_class = 
           predict(olive_lda_fit, olive_train)$.pred_class)
olive_lda_tr_pred %>% 
  count(region, .pred_class) %>%
  pivot_wider(names_from = region, 
              values_from = n, values_fill=0) %>%
  kable() %>%
  kable_styling(full_width = FALSE)


```

**The Linear Discriminant Matrix of the Olive test data **
```{r}

olive_lda_pred <- olive_test %>%
  mutate(.pred_class = 
           predict(olive_lda_fit, olive_test)$.pred_class)
olive_lda_pred %>% 
  count(region, .pred_class) %>%
  pivot_wider(names_from = region, 
              values_from = n, values_fill=0) %>%
  kable() %>%
  kable_styling(full_width = FALSE)


```

The following are the test error. 
```{r}
metrics(olive_lda_pred, truth = region, 
        estimate = .pred_class) %>%
  kable() %>%
  kable_styling(full_width = FALSE)

```

**This plot below have already showed the Boundary under the LDA prediction method: **

> In this question, we plot the boundary under training data for the reason that the training data have more observations compared to the test data, and the boundary under training data is also more precise than the boundary in test data.

```{r,echo=FALSE}
nd.x = seq(from = min(olive_train$arachidic), to = max(olive_train$arachidic), length.out =500)
nd.y = seq(from = min(olive_train$linoleic), to = max(olive_train$linoleic), length.out = 500)





grid <- expand_grid(arachidic = nd.x, linoleic = nd.y)



# pred area  


olive_grid <- grid %>% 
  mutate(.pred_class_lda = 
           predict(olive_lda_fit, grid)$.pred_class)





p10 <- ggplot(olive_grid) +
  geom_point(aes(x=arachidic, y=linoleic, 
                       colour=.pred_class_lda)) + 
  geom_point(data = olive_train,   # olive_train or olive_test ??
             aes(x=arachidic, y=linoleic, shape = region)) +
  scale_colour_brewer(palette="Dark2") +
  theme_bw() + ggtitle("LDA")


p10
```


**(ii)* classification tree (using `minsplit` of 10) using the `rpart` engine.**



```{r,echo=FALSE}


lda_model <- lda(region~., data=olive_train)
#prd = as.numeric(predict(lda_model, newdata = grid)$class)+1
```




**Here we use 'rpart' engine to construct the tree, from the following table,we can conclude that there rare 167 roots, there are totally 2 classes are separated. In the class 2 , 66 roots are put in and selection criteria is linoleic>=0.5366322.In class 3, there are totally 101 roots are selected in and criteria is linoleic< 0.5366322. **

```{r,echo=FALSE}


# set up the train prediction 

library(rpart)
tree <- rpart(region ~ .,method = "class",data = olive_train,control =rpart.control(minsplit = 10))
tree
```

```{r,echo=FALSE}

library(rattle)

```


```{r,echo=FALSE}
library(rpart.plot)
library(rattle)
prp(tree, type = 3, ni = TRUE, 
    nn = TRUE, extra = 2, box.palette = "RdBu")

fancyRpartPlot(tree)

```



>In this graph below, we can tell there is a green line made under *Classification Tree* Method when we use the rpart engine and well separated 2 datasets.


```{r}
ggplot(olive_train,aes(x=arachidic,y=linoleic,colour=region))+geom_point() +  # olive_train or olive_test ??
  scale_color_brewer("", palette="Dark2") +
  geom_hline(yintercept=0.5366322,colour="green")+ labs(caption = "Green line is Tree Boundrary")
```



### c. (1pt) Compute the test balanced accuracy for the three models (i) LDA,(ii) QDA, (iii) classification tree. Are they equally as accurate?



```{r,echo=FALSE}
olive_test_data <- olive_test[,2:3]
olive_test_label <- olive_test[,1]
```

```{r,echo=FALSE}
prd_2 <- predict(tree,olive_test_data)
prd2 <- c()
for (i in 1:nrow(prd_2)){
    if(prd_2[i,1]==1){
        prd2 <- append(prd2,2)
    }
    else{
        prd2 <- append(prd2,3)
    }
}

```

```{r,echo=FALSE}
prd = as.numeric(predict(lda_model, newdata = olive_test_data)$class)+1
```

```{r,echo=FALSE}
number_df <- nrow(olive_test_data)
```

```{r,echo=FALSE}
qda_model <- qda(region~., data=olive_train)
```

```{r,echo=FALSE}
prd_lda = as.numeric(predict(lda_model, newdata = olive_test_data)$class)+1
prd_tree = prd2
prd_qda = as.numeric(predict(qda_model, newdata = olive_test_data)$class)+1
```

```{r}
df_three_methods = data.frame(truth=olive_test_label[[1]],predict_lda=prd_lda, predict_tree=prd_tree,predict_qda=prd_qda)
```


**Below are the degree of freedom of 3 methods**
```{r,echo=FALSE}
df_three_methods[df_three_methods$truth==3 & df_three_methods$predict_tree==3,]
```

**Here we can show that there are totally rows of 32: **
```{r}
nrow(df_three_methods[df_three_methods$truth==2,])
```


**Then we calculate the balanced accuracy as follows: **
```{r}
# For LDA
ba_ac_lda = (nrow(df_three_methods[df_three_methods$truth==2 & df_three_methods$predict_lda==2,])/nrow(df_three_methods[df_three_methods$truth==2,])+nrow(df_three_methods[df_three_methods$truth==3 & df_three_methods$predict_lda==3,])/nrow(df_three_methods[df_three_methods$truth==3,]))/2
# For QDA
ba_ac_qda = (nrow(df_three_methods[df_three_methods$truth==2 & df_three_methods$predict_qda==2,])/nrow(df_three_methods[df_three_methods$truth==2,])+nrow(df_three_methods[df_three_methods$truth==3 & df_three_methods$predict_qda==3,])/nrow(df_three_methods[df_three_methods$truth==3,]))/2
# For tree
ba_ac_tree = (nrow(df_three_methods[df_three_methods$truth==2 & df_three_methods$predict_tree==2,])/nrow(df_three_methods[df_three_methods$truth==2,])+nrow(df_three_methods[df_three_methods$truth==3 & df_three_methods$predict_tree==3,])/nrow(df_three_methods[df_three_methods$truth==3,]))/2
```

```{r,echo=FALSE}
ba_ac_lda
ba_ac_qda
ba_ac_tree
```

All of them are equally as accurate. 
The 
$$Baac_{lda}= Baac_qda = Baac_{tree}$$

this means all of these 3 models have performed well in seperating different classes, as the balanced accuracy for each model are equal to 1 (100%) .  




### d. (1pt) Re-fit the three models with the additional variable `oleic` (so you now have three predictors). Write a paragraph discussing how the models change, and why this might be. ?



>Here, first of all we should take a look of the confusion matrix of all the 3 models. 




```{r,echo=FALSE}
olive2 <- read_csv("http://ggobi.org/book/data/olive.csv") %>%
  dplyr::filter(region != 1) %>%
  dplyr::select(region, arachidic, linoleic, oleic) %>%
  dplyr::mutate(region = factor(region))


#standardization
std<-function(x) (x-mean(x,na.rm=T))/sd(x, na.rm=T)
olive_std <-olive2 %>% 
  mutate_if(is.numeric,std)




set.seed(775)
olive_split1 <- initial_split(olive_std, 2/3, strata = region)
olive_train1 <- analysis(olive_split1)
olive_test1 <- assessment(olive_split1)

olive_qda_fit <- qda(region ~ ., 
      data = olive_train1)
olive_lda_fit <- lda(region ~ ., 
      data = olive_train1)
olive_tree_fit <- rpart(region ~ ., data = olive_train1, 
                 parms = list(split = "information"), 
                 control = rpart.control(minsplit = 10))
```









```{r,echo=FALSE}

# LDA model compared as before 
lda_model2 <- lda(region~arachidic+linoleic, data=olive_train1)

#lda_model22 <- lda(region~., data=olive_train2) ??? 
```


**Here we reform the LDA model with oleic as follows: **
```{r,echo=FALSE}
lda_model2
```

```{r,echo=FALSE}
#QDA model compared as before 
qda_model2 <- qda(region~arachidic+linoleic, data=olive_train1)
#bal_accuracy(qda_model2,region,pre)

```


**We also reform the QDA model with variable oleic **
```{r,echo=FALSE}
qda_model2

```

**We reform tree model after we added the oleic **

```{r,echo=FALSE}

#Tree model 
rpart(region ~ .,method = "class",data = olive_train1,control =rpart.control(minsplit = 10))
```






```{r,echo=FALSE}
library(rpart.plot)


```

```{r,echo=FALSE}
prp(olive_tree_fit, type = 3, ni = TRUE, 
    nn = TRUE, extra = 2, box.palette = "RdBu")



```

**The matrix for LDA **
```{r,echo=FALSE}
### lda
olive_test_lda <- olive_test1 %>%
  mutate(pred = predict(olive_lda_fit, olive_test1)$class)
conf_mat(olive_test_lda, region, pred)

bal_accuracy(olive_test_lda, region, pred) %>% 
  kable() %>%
  kable_styling(full_width = FALSE)

```



**Matrix for QDA **
```{r,echo=FALSE}
### qda
olive_test_qda <- olive_test1 %>%
  mutate(pred = predict(olive_qda_fit, olive_test1)$class)
conf_mat(olive_test_qda, region, pred)

bal_accuracy(olive_test_qda, region, pred) %>% 
  kable() %>%
  kable_styling(full_width = FALSE)

```

**Here, in comparison with the 3 models, based on balance accuracy, we can get that after added the other variable 'oleic'. The precision of LDA and tree model are same as without variable 'oleic'. This means that after adding an extra variable, the LDA and tree model are still accurate to estimate this new model. However, the accuracy of QDA is changed from 1 to 0.984375. Although this accuracy is still high, however, in comparison with the previous one, adding an extra variable made the balance accuracy of QDA model decreased by 1-0.984375=0.015626 **

# 2. (6pts) This question examines a classification model equation

The `palmerpenguins` is a new R data package, with interesting measurements on penguins of three different species. Subset the data to contain just the Adelie and Gentoo species, and only the variables species and the four physical size measurement variables. 

```{r,echo=FALSE}
library(palmerpenguins)
library(plotly)
library(tidyr)



library(tidyverse)
library(tourr)
library(rsample)
library(parsnip)
library(discrim)
library(yardstick)
library(spinifex)
library(MASS)
library(dplyr)




penguins1 <-penguins%>%
  mutate(species=factor(species))%>%
  dplyr::select(species,bill_length_mm:body_mass_g)


# loading the Data , here only adelie and gentoo are selected here 
penguins <-penguins%>%
  filter(species != "Chinstrap") %>%
  mutate(species=factor(species))%>%
  dplyr::select(species,bill_length_mm:body_mass_g)


# use the dplyr to specify the package 


# Standardizing the data 

std<- function(x) (x-mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)
penguins_std <- penguins %>%
  mutate_if(is.numeric, std) %>%
  drop_na() %>%
  rename(bl = bill_length_mm,
         bd = bill_depth_mm,
         fl = flipper_length_mm,
         bm = body_mass_g)
```




### a. (1pt) Make a scatterplot matrix of the data, with species mapped to colour. Which variables would you expect to be the most important for distinguishing between the species? Is it fair to assume homogeneous variance-covariances?  ?????

```{r,echo=FALSE}
library(GGally)
library(ggplot2)
```




```{r,echo=FALSE}
ggscatmat(penguins,columns=2:5,color="species")

```

**According to the scatter plot, we should select the plot when 2 area are tend to be more close, also within each group, the data should show a great correlation with each other.** 





```{r,echo=FALSE}

library(skimr)
#pc = penguins %>% filter() %>% dplyr::select( -one_of("island", "year") )

pc = penguins_std%>% filter() %>% dplyr::select( -one_of("island", "year") )
head(pc)
dim(pc)

# plot the box plot using caret package 

#caret::featurePlot(x = pc[, c("bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g")], 
 #                  y = pc$species ,
 #                  plot = 'box' , 
  #                scales = list(y = list( relation="free")))

caret::featurePlot(x = pc[, c("bl", "bd", "fl", "bm")], 
                   y = pc$species ,
                   plot = 'box' , 
                   scales = list(y = list( relation="free")))

library(car)
leveneTest(bl ~ species, penguins_std)
leveneTest(bd ~ species, penguins_std)
leveneTest(fl ~ species, penguins_std)
leveneTest(bm ~ species, penguins_std)



```


>Here, by observing the quantile performance, we can get the bl and fl separate 2 species well. 




**Then we can get more detailed look of by applying "featurePlot" analysis to have a look at the quantitative variable across the 2 species.** 

**We clearly observe that median and interquartile ranges of the 2 penguin species are quite separated , though by different features in each species. By separation, the means and interquartile range of one class is distinct and don't overlap with the other classes. **

**Here the variable bl and fl are estimated to separate the data well. **


#### bl
**From scatter plot, the data points are well separated with any combination that contains bl,which means the bill length may significantly different between 2 species.  **

**Moreover, from barplot, the overlap between 2 species under bl are also small, which means 2 classes are more separated under bl criteria **


#### fl 

**For fl inside the boxplot we can tell it has the lowest overlap among all the 4 variables. Meaning that for different species, there is a large difference between them. For scatterplot for fl, even though they are not as good as what bl displayed, since only very few data are mixed, we can still tell the difference among the 2 classees.**


>It's fair to assume the variance and covariance for each group as homogeneous for the reason that according to the plot. They basically have same shape among all options here.


**For the reason, Homogeneous variance-covariance means that each group has the same shape. The variance can be different in different projections, but it is the same for each group.It's fair to assume the variance and covariance for each group as homogeneous for the reason that according to the plot , for this implies that variables from different datasets may have common or similar variance (for data contain Adelie only and data contain Gentoo only)**






### b. (1pt) Break the data into training and test sets. Fit an LDA model to the data, assuming equal prior probabilities for the two groups. Report confusion table and the misclassification error for the test set. 


```{r,echo=FALSE}

library(tidyverse)
library(tourr)
library(rsample)
library(parsnip)
library(discrim)
library(yardstick)
library(spinifex)






# Break tha data into training and test area 

```



```{r,echo=FALSE}


set.seed(775)
# Break training data into training and test area 

# here whether we should use the standardized data ??? 
penguin_split <- initial_split(penguins_std,2/3,strata=species)
penguin_train <- training(penguin_split)
penguin_test <- testing (penguin_split)

## Fit the LDA model 


library(MASS)

``` 

```{r,echo=FALSE}

penguin_LDA = lda(species ~. , data=penguin_train,prior=c(0.5,0.5))
penguin_LDA
predictions_LDA <- penguin_LDA%>%predict(penguin_test)
```



**confusion matrix **

```{r,echo=FALSE}
library(caret)
(cm_lda_comp = confusionMatrix(predictions_LDA$class, penguin_test$species))



```
**Here the model appears to be accurate. THe overall accuracy is close to 1 and nearly 1 kappa on the test data. **



**According to the calculation method of the model error.**


**We can get that in the current dataset, everything matched well:**
```{r}

 error <- (0+0)/(51+40)
 error
```







### c. (1pt) Write down the LDA rule, for classifying the two species, explicitly being clear about which species is class 1 and which is class 2. ?????



**According to the LDA rule we can get that the following equation.New observation of X0 us belongs to class 1 if  **


$$
x_0^T\underbrace{\Sigma^{-1}(\mu_1-\mu_2)}_{Dimension~Reduction}>\frac12(\mu_1+\mu_2)^T\underbrace{\Sigma^{-1}(\mu_1-\mu_2)}_{Dimension~Reduction}
$$



**In LDA's definition, Class 1 and 2 need to be mapped in the your data. The class to the right on the reduced dimension will be class 1 in the equqation. So here after comparing the value with Adelie Penguin and Gentoo Penguin**


$$
Adelie \ should \ be \ Class \ 1 \ in \ this\  question\ if \ the \ above \ rule \ satisfies
$$
**here are the means and variables are displayed here ** 
```{r,echo=FALSE}


penguins_pred_mea<-penguin_LDA$means %*% as.matrix(penguin_LDA$scaling)


#penguins_LDA_1<-lda_mod%>%fit(species~.,data=penguin_train,prior=c(0.5,0.5))

Adelietrain<-penguin_train%>%filter(species=="Adelie")
Gentootrain<-penguin_train%>%filter(species=="Gentoo")

penguin_LDA$means %*% as.matrix(penguin_LDA$scaling)

mean_positive<- tibble("variables"=c("BL","BD","FL","BM"),
                       "Different mean between group -" =c(mean(Adelietrain$bl)-mean(Gentootrain$bl),
                                                           mean(Adelietrain$bd)-mean(Gentootrain$bd),
                                                           mean(Adelietrain$fl)-mean(Gentootrain$fl),
                                                           mean(Adelietrain$bm)-mean(Gentootrain$bm)))

mean_negative<- tibble("variables"=c("BL","BD","FL","BM"),
                       "Different mean between group +" =c(mean(Adelietrain$bl)+mean(Gentootrain$bl),
                                                           mean(Adelietrain$bd)+mean(Gentootrain$bd),
                                                           mean(Adelietrain$fl)+mean(Gentootrain$fl),
                                                           mean(Adelietrain$bm)+mean(Gentootrain$bm)))

mean_positive
mean_negative

```






### d. (1pt) Report the group means and the pooled variance-covariance matrix, and show the computations to obtain the linear discriminant space is computed from these. (Hint: You can use the `matlib` package to compute matrix inverses, see help [here](https://cran.r-project.org/web/packages/matlib/vignettes/inv-ex1.html).)


**We select the group means:**

```{r,echo=FALSE}
penguin_LDA$means
mua<- penguin_LDA$means[1,]
mub<-penguin_LDA$means[2,]



```

**Here we calculate the variance for each group **
```{r,echo=FALSE}
vara<-var(penguin_train[penguin_train$species=="Adelie",-1])
varb<-var(penguin_train[penguin_train$species=="Gentoo",-1])
```


**The variance of group Adelie**
```{r,echo=FALSE}
vara

```
**The variance of group Gentoo**

```{r,echo=FALSE}
varb
```

**Then we got the sum numbers of Adelie and Gentoo which are **
```{r,echo=FALSE}
n1<-sum(penguin_train$species=="Adelie")
n2<-sum(penguin_train$species=="Gentoo")
```

**According to the formula, we substitute the sum of numbers above into the pooled variance calculation we can get that **
```{r,echo=FALSE,results='hide'}

library(pracma)


```

**Here is the pooled variance-covariance matrix**
```{r}
var_pool<-((n1-1)*vara+(n2-1)*varb)/(n1+n2-2)
#var_pool

#solve(var_pool)%*%(mua-mub)
inv(var_pool)%*%(mua-mub)
```



### e. (1pt) Make a plot showing the data in the discriminant space, to examine how well the species are separated.

```{r,echo=FALSE,results='hide'}

#load the packages 

library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(GGally)
library(caret)
library(e1071)
library(klaR)
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)

library(palmerpenguins)
library(skimr)

```


```{r,echo=FALSE,results='hide'}


penguin_LDA$x


setA <- bind_cols(penguin_test, as_tibble(predict(penguin_LDA, penguin_test)$x))

```


**now we make a plot about the discriminant space of setA. **

```{r,echo=FALSE}
library(ggplot2)
ggplot(setA,aes(x=LD1,y=LD1,colour=species))+geom_point()+scale_colour_brewer("",palette="Dark2")+theme(aspect.ratio=1)
summary(setA)
```



**Here, by using an density function we can also get that **



```{r,echo=FALSE}
penguins_std_tr_pred <- predict(penguin_LDA, penguin_train)
penguins_std_ts_pred <- predict(penguin_LDA, penguin_test)


library(ggplot2)
penguins_std_tr <- penguin_train %>% 
  mutate(ld1=penguins_std_tr_pred$x, 
            Type=penguins_std_tr_pred$class, 
            set="train")
penguins_std_ts <- penguin_test %>% 
  mutate(ld1=penguins_std_ts_pred$x, 
            Type=penguins_std_ts_pred$class, 
            set="test")
penguins_std_fit <- bind_rows(penguins_std_tr, penguins_std_ts)
ggplot(penguins_std_fit, aes(x=ld1, fill=Type)) +
  geom_density()



ggplot(penguins_std_fit, aes(x=ld1,fill=species)) +
  geom_histogram(binwidth =0.5)+
  geom_density()+
  facet_grid(set ~ species)
# +geom_vline(xintercept = sum(penguins_std_mean_pred)/2, linetype =2)



```


**According to the density plot aren't intersect with each other, that means there is a good separation of the 2 classes of the Linear Discriminant method. **







### f. (1pt) Return to the data with the original three species of penguins. Conduct LDA for the three groups, and obtain the 2D discriminant space, using just the same four physical measurements. Using the tour, determine which of the variables are most important for distinguishing Chinstrap penguins from Adelie. Provide a plot or two to support your decision.  





```{r,echo=FALSE}

library(tidyverse)
library(ggplot2)
library(knitr)
library(kableExtra)
library(GGally)
library(caret)
library(e1071)
library(klaR)
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE)

library(palmerpenguins)
library(skimr)




std<- function(x) (x-mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)
penguins_std1 <- penguins1 %>%
  mutate_if(is.numeric, std) %>%
  drop_na() %>%
  rename(bl = bill_length_mm,
         bd = bill_depth_mm,
         fl = flipper_length_mm,
         bm = body_mass_g)

# The final dataset excludes observations with missing sex and drops the island and year variables.

pc = penguins_std1 %>% filter() %>% dplyr::select( -one_of("island", "year") )

head(pc)
dim(pc)





```


** Box plot and density plot can clearly show the relationship between four variables and easily tell
which variable are important for distinguishing**

```{r}
# Feature Plot of box plots
caret::featurePlot(x = pc[, c("bl", "bd", "fl", "bm")], 
                   y = pc$species ,
                   plot = 'box' , 
                   scales = list(y = list( relation="free")))

```

```{r}

# Feature Plot of density

caret::featurePlot(x = pc[, c("bl", "bd", "fl", "bm")], 
                   y = pc$species ,
                   plot = 'density' , 
                   scales = list(x = list( relation="free"), y = list(relation="free")) ,
                   auto.key = list( columns = 3 )
                   )



```








**TOur graph with 4 different dimensions. **


```{r,echo=FALSE}


library(tidyverse)
library(tourr)
library(rsample)
library(parsnip)
library(discrim)
library(yardstick)
library(spinifex)

penguins2<-penguins1%>%
  dplyr::select(species,bill_length_mm:body_mass_g)




set.seed(775)
penguin_split1 <- initial_split(penguins2, 2/3, strata = species)
penguin_train1 <- analysis(penguin_split1)
penguin_test1 <- assessment(penguin_split1)


penguins_lda_fit <- lda(species ~ .,data = penguin_train1)



setA2 <- penguins_lda_fit

setB2<- bind_cols(penguin_train1,as_tibble(predict(penguins_lda_fit,penguin_train1)$x))





setA_lda <- lda(species~., data=penguins1, prior = c(1,1,1)/3)

setA_all <- bind_cols(penguins1, as_tibble(predict(setA_lda, penguins1)$x))
p6<-ggplot(setA_all, aes(x=LD1, y=LD2, colour=species)) +
  geom_point() + 
  scale_colour_brewer("", palette="Dark2") +
  theme(aspect.ratio=1)
p6






library(ggplot2)

ggplot(setB2,aes(x=LD1,y=LD1,colour=species))+geom_point()+scale_colour_brewer("",palette="Dark2")+theme(aspect.ratio=1)


library(discrim)
library(yardstick)
library(spinifex)

library(rsample)
library(parsnip)
library(tidyverse)
library(tourr)


std<- function(x) (x-mean(x, na.rm=TRUE))/sd(x, na.rm=TRUE)



penguins_std1 <- penguins2 %>%
  mutate_if(is.numeric, std) %>%
  drop_na() %>%
  rename(bl = bill_length_mm,
         bd = bill_depth_mm,
         fl = flipper_length_mm,
         bm = body_mass_g)



lda_mod<-discrim_linear()%>%
  set_engine("MASS",prior=c(1/3,1/3,1/3))%>%
  translate()


penguins_lda_fit<- 
  lda_mod%>%
  fit(species~.,data=penguin_train1)

penguins_lda_fit


bas <- penguins_lda_fit$fit$scaling

play_manual_tour(bas, penguins_std1[,2:5], 1, 
                 aes_args = list(color = penguins_std1$species)) 
play_manual_tour(bas, penguins_std1[,2:5], 2, 
                 aes_args = list(color = penguins_std1$species))

play_manual_tour(bas, penguins_std1[,2:5], 3, 
                 aes_args = list(color = penguins_std1$species))

play_manual_tour(bas, penguins_std1[,2:5], 4, 
                 aes_args = list(color = penguins_std1$species))
```

**Here we can conclude after taking 'chinstrap' into consideration, we can get that bd are easy to tell the difference in order to separate Gentoo from Adelie and Chinstrap. Moreover, the value of bm can also separating Gentoo from Adelie and Chinstrap. **

**Furthermore, in such situation, we should always foruc on the tour to choose which variable are useful to separate species between Chinstrap and Adelie. According to the tour above, the green point is Adelie , whileas the orange point is chinstrap.According to the tour we can easily tell the bl is the most important criteria for the reason that after the projection, majority of green and orange cannot project on the bl. Based on this witness, we can say bl will retain important data for Chinstrap **

#  3. (5pts) This question examines impurity metrics for a classification tree

Impurity metrics can be other than Gini or entropy. This is a metric proposed by [Buja and Lee (2001)](https://repository.upenn.edu/cgi/viewcontent.cgi?article=1406&context=statistics_papers) is called a one-sided extreme:

$$
OSE = 1-max(\hat{p}_L^1, \hat{p}_R^1)
$$
and is designed to do as well as possible in splitting off pure subsets of class 1. That is, it might help in a problem where one class is very important to predict well. 

We are going to use this metric to build a spam filter for the spam data used in tutorial 4.


## a. (0.5pt) Read in the spam data, set the levels for day of the week, and filter to the five most common domains, "com", "edu", "net", "org", "gov". Drop variable spampct, because there are many missing values. (This variable was one that some of the mail software provided as a probability that the email was spam. Most mail software didn't provide this.) Class 1 for this data would be `spam=no`. Explain why.  

**According to the rule of thumb, put the most important classes among the 2 classes into class one. For the reason the program asked us about separating spam, so 'spam=no' is more important, thus we put it into class 1. **

```{r,echo=F,results='asis'}
library(dplyr)
```




```{r,echo=FALSE,results='hide'}
spam <- read_csv("http://ggobi.org/book/data/spam.csv") %>%
  mutate(`day of week` = factor(`day of week`,
              levels=c("Mon", "Tue", "Wed", "Thu",
                       "Fri", "Sat", "Sun"))) %>%
  filter(domain %in% c("com", "edu", "net", "org", "gov"))%>%# filter to the five most common domains. 
# python in 
  dplyr::select(-spampct)




```



### Here Below are the Spam Data we will later used 
**After remove the spampct column **

```{r}
spam
```






### b. (1pt) Considering only the domain variable, what would be all of the possible splits? Compute OSE for each possible split, and report the best split, by hand.


**For all the possible split, the graph will be as follows. For each left and right bucket, will have 15 splits(rows).Each row of left are combined together with right are original 5 domain variables. **



```{r,echo=FALSE}
d <- unique(spam$domain)
```

```{r,echo=FALSE}
get_5_2 <- gtools::permutations(5,2) # permutation different dataset from left to right 
```

```{r}
# Decreasing sorting of the dataset to get the ideal permutation.
for (i in 1:nrow(get_5_2)){
    get_5_2[i,] <- sort(get_5_2[i,])
}


get_5_2
```

```{r,echo=FALSE}
get_5_2 <- unique(get_5_2)
# remove the common items in the permutation list 

```

```{r,echo=FALSE}
get_5_1 <- c(1,2,3,4,5)
```

```{r,echo=FALSE}
left <- list()
right <- list()
```

```{r,echo=FALSE}

#calculate the 1:4 split 
for (left_ind in get_5_1){
    left <- append(left,list(d[left_ind]))
    right <- append(right,list(d[-left_ind]))
}
```

```{r,echo=FALSE}
# calculate the 2:3 split 

for (left_ind in 1:nrow(get_5_2)){
    left_in <- get_5_2[left_ind,]  #??? 
    left <- append(left,list(d[c(left_in)]))
    right <- append(right,list(d[-c(left_in)]))
}

```

```{r,echo=FALSE}

da_q3 <- tibble(index=seq(1,15,1),ose=0)
```

```{r,echo=FALSE}
pr.l = spam[spam$domain %in% left[[1]],]
```

```{r,echo=FALSE}
nrow(spam[spam$domain %in% left[[1]],])
nrow(spam)
```


**Here we can get the right bucket will be: **
```{r,echo=FALSE}
right
```

```{r,echo=FALSE}


for (i in da_q3$index){
    left_name = left[i][[1]]
    right_name = right[i][[1]]
    left_spam = spam[spam$domain %in% left_name,]
    right_spam = spam[spam$domain %in% right_name,]
    pr.l = nrow(spam[spam$domain %in% left_name & spam$spam=="no",])/nrow(left_spam)
    pr.r = nrow(spam[spam$domain %in% right_name & spam$spam=="no",])/nrow(right_name)
    da_q3[i,2] = 1-max(pr.l,pr.r)
}


```

**The Left bucket will be **
```{r,echo=FALSE}
left
```


**Then the tested balance will be **
```{r,echo=FALSE}
da_q3
```
**Inside this list, among all 15 observations, the 2nd observation has the least ose which is 0.008679. **



**We should choose 0.008679 as the best split where the index of this split is 2. In this case we can get that inside the left bucket will have domain as com,gov,net,org , in the right bucket will have the domain with only edu. **


### c. (1.5pt) Write a function to compute OSE, given a numeric variable. It needs to have an option for minimum split, so that you can restrict the minimum size for each subset. At what value would of `size.kb` would the split be made? (Using a minimum split value of 10.)



```{r,echo=FALSE}
ose_func = function(variable,minsplit) {
  min=sort(variable,FALSE)[minsplit]
  max=sort(variable,TRUE)[minsplit]
  max=sort(variable[variable<max],T)[1]
  
  optimal_ose=1
  optimal_ose_v=1
  
  for(i in min:max){
  left=filter(cbind(variable,spam),variable<=i)
  left_ose=sum(left$spam=="no")/length(left$spam)
  right=filter(cbind(variable,spam),variable>i)
  right_ose=sum(right$spam=="no") / length(right$spam)
  ose= 1-max(left_ose,right_ose)
    
  if(ose<optimal_ose){
    optimal_ose=ose
    optimal_ose_v=i
      
    }
    }
  return_value=cbind(optimal_ose,optimal_ose_v)
  return(return_value)
}


ose_func(spam$size.kb,10)
```

**Here after we called the function we set, we can get that when the size.kb is equal to 113, the OSE under *minsplit=10* is least among all the other groups. **

**For the optimal_ose is equals to 0, there maybe have decimals haven't been displayed here, so we can conclude  under size = 113, the optimal_ose is least among all the other group. **






### d. (1pt) Use your function to compute OSE for each possible split for the other numeric variables (time of day, digits, cappct, special). Which of these would produce the best split? And what is the split?

$$
OSE = 1-max(\hat{p}_L^1, \hat{p}_R^1)
$$

**Below are the columns names of the spam data. **

```{r,echo=FALSE}
colnames(spam)
```



**We use the ose function to calculate OSE for variable "time of the day" **
```{r,echo=FALSE}
ose_func(spam$`time of day`,10)
```



**We use the function to calculate the variable "special" **
```{r}
ose_func(spam$special,10)
```


*Here, we use the function to calculate the OSE of variable "digits"* 
```{r}
ose_func(spam$digits,10)
```


*Here, we use the function to calculate the OSE of variable "cappct"*
```{r}
ose_func(spam$cappct,10)
```

**As for the calculation above, we can apply the function we set above, we can conclude that the best split will be generated by variable special, with the value of optimal OSE is 0.2638581. Here, for the reason that there are no decimal been kept and special can only be integer . Thus, as for varialbe 'special' only, when special is zero we can get the optimal_ose and the ose value when variable 'special' is zero is 0.2638581 **



### e. (1pt) Make a plot of the Gini measure for impurity, against all values of $p$ (0-1), Overlay the OSE index. Compare the two measures, based on the worst and best $p$ for each.  

 
**The Gini measure formula will be **
$$
G=\Sigma^k _{k=1}\hat p_{mk}(1-\hat p_{mk})
$$
**In this question, $\hat p_{mk}= p $ thus $(1-\hat p_{mk})=(1-p)$. If $p$ is left bucket then $1-p$ is right bucket. Vice Versa. **



**For OSE the function is **

$$
OSE = 1-max(\hat{p}_L^1, \hat{p}_R^1)
$$

**For Gini index , the lower value the better measurement from the following graph we can conclude that when $P =0.5$ is the worst, while $P=0$ or $P=1$, Gini index will perform better. **


**For OSE , the lower value of OSE means the better model is. When the $P=0$ OSE will be very small, which is the optimal split, when $p=1$ , the value of OSE is high, which is worse split**

```{r}
p <- seq(0, 1, length.out = 1000)
gini <- p*(1-p)
ose <- 1 - pmax(1-p)
#entropy <- -(p*log(p) + (1-p)*log(1-p))
plot(p, gini, type = "l", ylim = c(0,0.7))
lines(p, ose, lty = 2, col = 2)
#lines(p, entropy, lty = 3, col = 3)
```




### 4. (5pts) Conducting and interpreting a PCA

Principal component analysis is often used to create indicator variables (see e.g. [Constructing socio-economic status indices: how to use principal components analysis](https://academic.oup.com/heapol/article/21/6/459/612115)). In this question, you will look at the socioeconomic data [provided on kaggle](https://www.kaggle.com/ashydv/country-socioeconomic-data/discussion/155753) to create an indicator variable, using PCA.

### a. (0.5pt) How many PCs are possible to compute on this data?

```{r,echo=F,results="FALSE"}
library(readr)
soecdata <-read_csv('C:/Users/elvis/Desktop/R Projects/ETC 3250/Assignment 2/data/socio-economic-kaggle.csv')

```



```{r,echo=FALSE}
# Display a structure of an arbitrary of R object 
str(soecdata)
```

**We can conclude there are 9 PCs need to be calculated for there are totally 9 variables.**




### b. (1pt) Compute a PCA. What proportion of variance does the first PC explain?

```{r,echo=FALSE}
#compute the PCA using prcomp function 
library(GGally)
ggscatmat(soecdata[,1:10])

pca4 <- prcomp(soecdata[,2:10],center=TRUE,scale=TRUE)
pca4
```
**According to the table above, there are totally 9PCs are possible to calculate in this data. **


```{r,echo=FALSE}

library(kableExtra)
library(knitr)

soac_summary<- tibble(evl=pca4$sdev^2)%>%
  mutate(p=evl/sum(evl),cum_p=cumsum(evl/sum(evl)))%>% t()

colnames(soac_summary) <-colnames(pca4$rotation)

rownames(soac_summary) <- c("variance", "peopoerion", "Cum.prop")

kable(soac_summary,digits=4,align = "r") %>%
  kable_styling(full_width=T)%>%
  row_spec(0,color = "white", background="#7570b3")%>% 
  column_spec(1,width="2.5em",color="white",background="#7570b3")%>%
  column_spec(1:8,width="2.5em")%>%
  row_spec(3,color = "white",background="#CA6627")

```

#### First principal component

The first principal component of a set of variables $x_1, x_2, \dots, x_p$ is the linear combination


$$
z_1 = \phi_{11}x_1 + \phi_{21} x_2 + \dots + \phi_{p1} x_p
$$



$$
PC1=  -0.4195194 \times child_{mort}+0.2838970 \times exports + 0.1508378 \times health+0.1614824 \times imports +0.3984411 \times income-0.1931729 \times inflation+0.4258394  \times  life_{expec}-0.4037290 \ times total_{fer}+ 0.3926448 \times gdpp
$$

**We can get the following summary as **

```{r,echo=FALSE}
summary(pca4)
```


>According to the graph, the proportion of variance explained by the first PC is 0.4595, that is 45.95% data are explained by PC1



### c. (1pt) Examine the loading for the PC1. Make a plot of the loading (like done during tutorial). 


The loading of PC1 is shown as follows. 
```{r,echo=FALSE}
soec_pc_loadings <- as_tibble(pca4$rotation[,1:2]) %>%
  mutate(date = rownames(pca4$rotation), 
         indx = 1:nrow(pca4$rotation),
         ymin=rep(0, nrow(pca4$rotation)))

pc1_plot <- ggplot(soec_pc_loadings) + 
  geom_hline(yintercept=c(-1/sqrt(nrow(pca4$rotation)),
                          1/sqrt(nrow(pca4$rotation))), colour="red") + 
  geom_errorbar(aes(x=indx, ymin=ymin, ymax=PC1)) +
  geom_point(aes(x=indx, y=PC1))

pc1_plot


```
**Based on the plot of loadings, there are lots of variability in the loadings coefficient and a bootstrap is requred to be used in order to generated a more precise and relevant variables.** 




### d. (1pt) Use bootstrap to assess which variables could be considered unimportant for PC1 (ie loading not significantly different from 0).

```{r,echo=FALSE}



library(boot)
compute_PC1 <- function(data, index) {
  pc1 <- prcomp(data[index,], center=TRUE, scale=TRUE)$rotation[,1]
  # Coordinate signs
  if (sign(pc1[1]) < 0) 
    pc1 <- -pc1 
  return(pc1)
}
# Make sure sign of first PC element is positive
PC1_boot <- boot(data=soecdata[,2:10], compute_PC1, R=1000)

colnames(PC1_boot$t) <- colnames(soecdata[,2:10])

PC1_boot_ci <- as_tibble(PC1_boot$t) %>%
  gather(var, coef) %>% 
  mutate(var = factor(var, levels=c("child_mort", "exports", "health", "imports", "income", "inflation", "life_expec", "total_fer", "gdpp"))) %>%
  group_by(var) %>%
  summarise(q2.5 = quantile(coef,0.025), 
            q5 = median(coef),
            q97.5 = quantile(coef, 0.975)) %>%
  mutate(t0 = PC1_boot$t0) 
  
ggplot(PC1_boot_ci, aes(x=var, y=-t0)) +  # Here child_mort should be negative correlated with the soec data status 
  geom_hline(yintercept=1/sqrt(9), linetype=2, colour="red") +
  geom_point() +
  geom_errorbar(aes(ymin=-q2.5, ymax=-q97.5), width=0.1) +  # Here child_mort should be negative correlated with the soec data status 
  geom_hline(yintercept=0, size=3, colour="white") +
  xlab("") + ylab("coefficient") + 
  geom_hline(yintercept=-1/sqrt(9), linetype=2, colour="red")
```

**According the graph we can conclude that on PC1, the following variables $child_{mort}$,$total_{fer}$,$income$ , $life_{expec}and $GDPP$. That means these variables are significantly diffferent from $0$,so in such situation we should take it as their means outside the confidence interval. Meanwhile, from the other aspect, the $exports$, $ health$, $imports$ and $inflation$ is not slightly different from $0$ and within confidence interval , however , non of them accross the "0" boundary. Thus, we can conclude these variables are less significance and can be rejected to fit a new model. **



### e. (0.5pt) Write down the formula for your new indicator variable. What would you consider it to be an indicator of? That is, interpret PC1 based on the loadings. 

```{r,echo=FALSE}
loadings<- as_tibble(pca4$rotation)
loadings%>%arrange(desc(PC1))
```

**We here after select the useful variables, we construct a newpca that is consisted of the useful variables here. **




```{r}
newpca<- prcomp(soecdata[,c(2,6,8,9,10)],center=T,scale=T)
newpca


```




```{r}
summary(newpca)
```

**The new formula of PC1 will be **

$$
PC1=  -0.4654859 \times child_{mort}+ 0.4297577 \times income +  0.4790410  \times  life_{expec}  -0.4420473 \times total_{fer}+ 0.4168274 \times gdpp
$$


** Here's the reason why we should choose PC1 as our indicator variable is first of all, PC1 has a Cumulative proportion 
 
**After we selected the significant variables, We have made a pca4 only consists **
$$
child_{mort},\ income , \ life_{expec} , \ total_{fer} ,\ gdpp
$$



### f. (1pt) Make a biplot of the first two PCs. Which few countries have the highest values, and which few countries have the lowest value on PC1? What does it mean for a country to have a high value on PC1? 

```{r eval=FALSE}
# here we gather 2PCs together in a table. We can get the following 
sc2 <- as_tibble(pca4$rotation[,1:2]) %>%
  mutate(date = rownames(pca4$rotation), 
         indx = 1:nrow(pca4$rotation),
         ymin=rep(0, nrow(pca4$rotation)))

sc2 

da_4 <- newpca$x %>%
          as_tibble() %>%
          mutate(country = soecdata$country)

```
 
 
 
 

```{r,echo=FALSE}
library(ggrepel) 



library(ggrepel)
soec_pca_pcs <- as_tibble(pca4$x[,1:2]) %>%
  mutate(cnt=soecdata$country)

pca4_var<- tibble(n=1:length(pca4$sdev),evl=pca4$sdev^2)




# here draw the graph 


soec_pca_evc <- as_tibble(pca4$rotation[,1:2]) %>% 
  mutate(origin=rep(0, 9), variable=colnames(soecdata[,c(2,3,4,5,6,7,8,9,10)]),
         varname=rownames(pca4$rotation)) %>%
  mutate(PC1s = PC1*(pca4_var$evl[1]*2.5), 
         PC2s = PC2*(pca4_var$evl[2]*2.5))



pca_p <- ggplot() + 
  geom_segment(data=soec_pca_evc, aes(x=origin, xend=PC1s, y=origin, yend=PC2s), colour="orange") +
  geom_text_repel(data=soec_pca_evc, aes(x=PC1s, y=PC2s, label=variable, nudge_y=sign(PC2)*0.1), colour="orange", nudge_x=0.1) +
  geom_point(data=soec_pca_pcs, aes(x=PC1, y=PC2)) +
  geom_text(data=filter(soec_pca_pcs, abs(PC1)>3.5), aes(x=PC1, y=PC2, label=cnt), nudge_y=0.1, nudge_x=-0.1) +
  geom_text(data=filter(soec_pca_pcs, abs(PC2)>2.2), aes(x=PC1, y=PC2, label=cnt), nudge_y=0.1, nudge_x=-0.1) +
  xlab("PC1") + ylab("PC2") +
  theme(aspect.ratio=1)
pca_p
```


**According to the Biplot,luxembourg,Singapore,Qatar , Malta are the 5 highest value of PC1. Meanwhile, Nigeria,Haiti,Central African Republic, chad are 4 countries with smallest PC1. Based on the macroeconomic theory, we can conclude the developed countries are tend to have high PC1, those developing countries with low development status will tend to have low PC1s. Also we can have a look of the coloured plot, we can conclude that high value of imports, exports, income , health , life_expec,gdpp will have positive impact on PC1. On the contrary, variables like child_mort, inflation,total_fer will also have a negative impact on PC1. **

**All in all, based on the two plots, we can say that a country with a high PC1 will also high values on imports, exports, income , health , life_expec,gdpp and low values on child_mort, inflation,total_fer, the countries are tend to be more developed. A country with low PC1 will have low values on  imports, exports, income , health , life_expec,gdpp and high value on child_mort, inflation,total_fer and tend to be less developed country**




  
```{r}

citation("ggrepel")
citation("tidyverse")
citation("tidymodels")
citation("discrim")
citation("MASS")
citation("boot")
citation("ggrepel")
citation("kableExtra")
citation("caret")
citation("dplyr")
citation("tourr")
citation("magrittr")
citation("ggplot2")
citation("rpart")
citation("rpart.plot")
citation("knitr")
citation("klaR")
citation("skimr")
citation("spinifex")
citation("yardstick")

citation("parsnip")
citation("rsample")
citation("plotly")
citation("GGally")
citation("pracma")
citation("car")
citation("rattle")
```

  
  


